{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Yuantong Ding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "A 2-layer CNN for MNIST digit classfication:\n",
    "\n",
    "Image -> convolution (32 5x5 filters) -> nonlinearity (ReLU) ->  (2x2 max pool) -> convolution (64 5x5 filters) -> nonlinearity (ReLU) -> (2x2 max pool) -> fully connected (256 hidden units) -> nonlinearity (ReLU) -> fully connected (10 hidden units) -> softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import trange\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "# Import data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(mnist.train.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape data\n",
    "#mnist_train_images_reshaped = tf.reshape(mnist.train.images, [-1,28,28,1])\n",
    "#print(mnist_train_images_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of conv1 feature maps after max pooling:(?, 14, 14, 32)\n",
      "Shape of conv2 feature maps after max pooling:(?, 7, 7, 64)\n",
      "Shape of flattened feature maps after max pooling 2:(?, 3136)\n",
      "Shape of latent_scores_1:(?, 256)\n",
      "Shape of final scores:(?, 10)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "tf.reset_default_graph()\n",
    "g = tf.get_default_graph()\n",
    "\n",
    "# Create image input placeholder\n",
    "X = tf.placeholder(tf.float32,[None,784])\n",
    "x_cnn = tf.reshape(X, [-1, 28, 28, 1])\n",
    "#x_cnn = tf.placeholder(tf.float32,[None,28,28,1]) # height, width, color\n",
    "\n",
    "y = tf.placeholder(tf.float32,[None,10]) # 10 classes of digits\n",
    "\n",
    "# Convolutional layer 1\n",
    "W1 = tf.Variable(tf.truncated_normal([5,5,1,32],stddev=0.1)) # 5x5 filter, 32 filters\n",
    "b1 = tf.Variable(tf.zeros([32])) # one per filter\n",
    "conv1_preact = tf.nn.conv2d(x_cnn, W1, strides=[1,1,1,1], padding=\"SAME\" ) + b1\n",
    "\n",
    "# ReLu 1\n",
    "conv1 = tf.nn.relu(conv1_preact)\n",
    "\n",
    "# 2x2 max pool 1\n",
    "max_pool1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\") #only to height and width, usually strides are the same with ksize\n",
    "print(\"Shape of conv1 feature maps after max pooling:{0}\".format(max_pool1.shape))\n",
    "\n",
    "# Convolutional layer 2\n",
    "W2 = tf.Variable(tf.truncated_normal([5,5,32,64],stddev=0.1)) # 5x5 filter, 32 features, 64 filters\n",
    "b2 = tf.Variable(tf.zeros([64])) # one per filter\n",
    "\n",
    "# Apply 2nd convolutional layer\n",
    "conv2_preact = tf.nn.conv2d(max_pool1, W2, strides=[1,1,1,1], padding=\"SAME\" ) + b2\n",
    "conv2 = tf.nn.relu(conv2_preact)\n",
    "\n",
    "# 2x2 max pool 2\n",
    "max_pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\") #only to height and width, usually strides are the same with ksize\n",
    "print(\"Shape of conv2 feature maps after max pooling:{0}\".format(max_pool2.shape))\n",
    "\n",
    "# fully connected 1\n",
    "flat = tf.reshape(max_pool2, [-1, 7*7*64])\n",
    "print(\"Shape of flattened feature maps after max pooling 2:{0}\".format(flat.shape))\n",
    "W3 = tf.Variable(tf.truncated_normal([7*7*64, 256], dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "b3 = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32), trainable=True, name='biases')\n",
    "latent_scores_1 = tf.nn.relu(tf.add(tf.matmul(flat,W3),b3)) \n",
    "print(\"Shape of latent_scores_1:{0}\".format(latent_scores_1.shape))\n",
    "\n",
    "# fully connected 2\n",
    "W4 = tf.Variable(tf.truncated_normal([256, 10], dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "b4 = tf.Variable(tf.constant(0.0, shape=[10], dtype=tf.float32), trainable=True, name='biases')\n",
    "scores = tf.nn.relu(tf.add(tf.matmul(latent_scores_1,W4),b4)) \n",
    "print(\"Shape of final scores:{0}\".format(scores.shape))\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=scores, labels=y) # softmax\n",
    "avg_loss = tf.reduce_mean(loss)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [09:32<00:00, 57.23s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for iter in trange(10):\n",
    "    for i in range(550):\n",
    "        batch_xs = mnist.train.images[i*100:(i+1)*100]\n",
    "        batch_ys = mnist.train.labels[i*100:(i+1)*100]\n",
    "        sess.run(train_step, feed_dict={X: batch_xs, y: batch_ys})\n",
    "\n",
    "# sess.run(train_step, feed_dict={X:mnist.train.images[0:100],\n",
    "#               y:mnist.test.labels[0:100]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9891"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "computed_scores = sess.run(scores, feed_dict={X:mnist.test.images,\n",
    "                      y:mnist.test.labels})\n",
    "np.argmax(computed_scores,axis=1)\n",
    "\n",
    "sum(np.argmax(computed_scores,axis=1)==np.argmax(mnist.test.labels,axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short answer\n",
    "\n",
    "1\\. How does the CNN compare in accuracy with yesterday's logistic regression and MLP models? How about training time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN has slightly higher accuracy (98.8%) than MLP (97.8%). However, CNN takes much longer time (~10min) to trian for the same number of iterations than MLP (~10s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. How many trainable parameters are there in the CNN you built for this assignment?\n",
    "\n",
    "*Note: By trainable parameters, I mean individual scalars. For example, a weight matrix that is 10x5 has 50.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution layer 1: weights + bias <br>\n",
    "5 \\* 5 \\* 32 + 32 = 832\n",
    "\n",
    "Convolution layer 2: weights + bias<br>\n",
    "5 \\* 5 \\* 32 \\* 64 + 64 = 51,264\n",
    "\n",
    "Fully connected layer 1: weights + bias<br>\n",
    "7 \\* 7 \\* 64 \\* 256 + 256 = 803,072\n",
    "\n",
    "Fully connected layer 1: weights + bias<br>\n",
    "256 \\* 10 + 10 = 2570\n",
    "\n",
    "Total: 857,738"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. When would you use a CNN versus a logistic regression model or an MLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are some repeated/shared sturctures exist in the data, like an image, it may be more reseanable to chose CNN over a simple MLP. Because CNN has translational invariance. Also, for large input, MLP model parameters would become impractically large while CNN's limited connections and weight sharing mean it can scale up much better than a pure MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
